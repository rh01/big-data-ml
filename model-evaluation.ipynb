{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this activity, you will be able to perform the following in Spark:\n",
    "\n",
    "Determine the accuracy of a classifier model\n",
    "Display the confusion matrix for a classifier model\n",
    "In this activity, you will be programming in a Jupyter Python Notebook. If you have not already started the Jupyter Notebook server, see the instructions in the Reading Instructions for Starting Jupyter.\n",
    "\n",
    "Step 1. Open Jupyter Python Notebook. Open a web browser by clicking on the web browser icon at the top of the toolbar:\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/RCneZE7PEeaqTxIkdCEfsw_c491f272226b35805e44abef7a7a22a9_browser-icon.png?expiry=1507852800000&hmac=Tm1M4Bmovuy3gsD3GcbtEhO0sFTqQnTc_CuVCnskA-w)\n",
    "\n",
    "Navigate to localhost:8889/tree/Downloads/big-data-4:\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/9Zu58oqhEeaKKwpaECzIKQ_361b99533aaa8d7cde3e3df56b69b3f5_browser.png?expiry=1507852800000&hmac=xdwPQiACz7sjm7XYxYhIJHxEGIQ13eig6Gzhc1rtnw4)\n",
    "Open the model evaluation notebook by clicking on model-evaluation.ipynb:\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/BfjlhovoEea63BLi-G7oTw_f95938970c4788aac115fe4c8866d0c6_notebook.png?expiry=1507852800000&hmac=suhudiH4e-Vj-aN_Cz4pTfCYC1zwLzmYvHnwCvgKZU4)\n",
    "\n",
    "Step 2. Load predictions. Execute the first cell to load the classes used in this activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to load the predictions CSV file that we created at the end of the Week 3 Hands-On Classification in Spark into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "predictions = sqlContext.read.load('file:///home/cloudera/Downloads/big-data-4/prediction.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Compute accuracy. Let's create an instance of MulticlassClassificationEvaluator to determine the accuracy of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",predictionCol=\"prediction\",metricName=\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassificationEvaluator_4d7f8e0d2dd97cb2d5fa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two arguments specify the names of the label and prediction columns, and the third argument specifies that we want the overall precision.\n",
    "\n",
    "We can compute the accuracy by calling evaluate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.81\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy = %.2g\" % ( accuracy ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Display confusion matrix. The MulticlassMetrics class can be used to generate a confusion matrix of our classifier model. However, unlike MulticlassClassificationEvaluator, MulticlassMetrics works with RDDs of numbers and not DataFrames, so we need to convert our predictions DataFrame into an RDD.\n",
    "\n",
    "If we use the rdd attribute of predictions, we see this is an RDD of Rows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=1.0, label=1.0), Row(prediction=1.0, label=1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can map the RDD to tuple to get an RDD of numbers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (1.0, 1.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.rdd.map(tuple).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of MulticlassMetrics with this RDD:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(predictions.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above command can take longer to execute than most Spark commands when first run in the notebook.\n",
    "\n",
    "The confusionMatrix() function returns a Spark Matrix, which we can convert to a Python Numpy array, and transpose to view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusionMatrix() function returns a Spark Matrix, which we can convert to a Python Numpy array, and transpose to view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87.,  26.],\n",
       "       [ 14.,  83.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**\n",
    "\n",
    "Spark: In the last line of code in Step 4, the confusion matrix is printed out. If the “transpose()” is removed, the confusion matrix will be displayed as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87.,  14.],\n",
       "       [ 26.,  83.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
